# Optimization
Once we have the results of the algorithms tested in the baseline, we will think about improving them.

To improve our models, we will use Hyperparameter Optimization, which will allow us to find the combination of parameters that will return the best possible model for each algorithm.

However, there may also be cases where our algorithms overlearn and the results are not as expected.

The algorithms on which the Hyperparameter Optimization has been carried out are the following:

- [KNN](https://github.com/sergiosb99/SSJ_SupervisedLearning/blob/Optimization/Dengue_KNN_Optimization.ipynb)
- [Decision Tree](https://github.com/sergiosb99/SSJ_SupervisedLearning/blob/Optimization/Dengue_DecisionTree_Optimization.ipynb)
- [Random Forest](https://github.com/sergiosb99/SSJ_SupervisedLearning/blob/Optimization/Dengue_RandomForest_Optimization.ipynb)
- [Boosting](https://github.com/sergiosb99/SSJ_SupervisedLearning/blob/Optimization/Dengue_Boosting_Optimization.ipynb)
- [AdaBoost](https://github.com/sergiosb99/SSJ_SupervisedLearning/blob/Optimization/Dengue_AdaBoost_Optimization.ipynb)
- [SVC](https://github.com/sergiosb99/SSJ_SupervisedLearning/blob/Optimization/Dengue_SVC_Optimization.ipynb)

